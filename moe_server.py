# moe_server.py
import argparse
import os
import time
import uuid
import torch
import torch.nn.functional as F
from typing import List, Optional, Literal, Generator, Union
from fastapi import FastAPI, Request
from fastapi.responses import JSONResponse, StreamingResponse
from pydantic import BaseModel
from collections import defaultdict
from transformers import AutoTokenizer, AutoModelForCausalLM, GenerationConfig
import datetime
import json

# ========== ç¯å¢ƒé…ç½® ==========
os.environ['DISABLE_MODELSCOPE_HUBUTILS'] = '1'
os.environ['WORLD_SIZE'] = '1'
os.environ['RANK'] = '0'
os.environ['LOCAL_RANK'] = '0'

# ========== å¸¸é‡ ==========
MODEL_PATH = "/home/nvidia/MoE/deepseek-ai/deepseek-moe-16b-chat"
DEVICE = "cuda"
DEVICE_ID = "0"
CUDA_DEVICE = f"{DEVICE}:{DEVICE_ID}" if DEVICE_ID else DEVICE
DEBUG_MODE = False

# ========== FastAPI åˆå§‹åŒ– ==========
app = FastAPI(title="DeepSeek MoE API Server", version="2.0")

# ========== è¯·æ±‚ç»“æ„å®šä¹‰ ==========
class ChatMessage(BaseModel):
    role: Literal["system", "user", "assistant"]
    content: str

class ChatRequest(BaseModel):
    model: str = "deepseek-moe-16b-chat"
    messages: List[ChatMessage]
    temperature: Optional[float] = 0.7
    max_tokens: Optional[int] = 512
    stream: Optional[bool] = False

class LegacyRequest(BaseModel):
    prompt: str
    max_length: Optional[int] = 512

# ========== å…¨å±€å˜é‡ ==========
model = None
tokenizer = None
expert_activations = []
hook_call_count = 0
hooks = []

# ========== GPUå†…å­˜æ¸…ç†å‡½æ•° ==========
def torch_gc():
    if torch.cuda.is_available():
        with torch.cuda.device(CUDA_DEVICE):
            torch.cuda.empty_cache()
            torch.cuda.ipc_collect()

# ========== Hookå‡½æ•° - ä¿ç•™åŸæœ‰çš„è¯¦ç»†è¿½è¸ªé€»è¾‘ ==========
def detailed_track_experts(module, input, output):
    """è¯¦ç»†çš„ä¸“å®¶æ¿€æ´»è¿½è¸ªå‡½æ•° - ä¸“é—¨é’ˆå¯¹DeepSeek MoEä¼˜åŒ–"""
    global hook_call_count
    hook_call_count += 1

    try:
        module_name = str(type(module).__name__)
        module_full_name = getattr(module, "_module_name", "unknown")

        # é‡ç‚¹å…³æ³¨MoEGateå±‚ - è¿™æ˜¯ä¸“å®¶é€‰æ‹©çš„æ ¸å¿ƒ
        is_moe_gate = module_name == "MoEGate"
        is_deepseek_moe = module_name == "DeepseekMoE"
        is_potential_moe = any(
            keyword in module_full_name.lower()
            for keyword in ["moe", "expert", "gate", "router"]
        )

        # å¯¹äºMoEç›¸å…³å±‚æˆ–å‰50ä¸ªè°ƒç”¨ï¼Œæ‰“å°è¯¦ç»†ä¿¡æ¯
        should_log_detail = (
            (hook_call_count <= 50 and DEBUG_MODE)
            or is_moe_gate
            or is_deepseek_moe
            or is_potential_moe
        )

        should_log_detail = False

        if should_log_detail:
            print(f"\n=== Hookè°ƒç”¨ #{hook_call_count} ===")
            print(f"æ¨¡å—ç±»å‹: {module_name}")
            print(f"å®Œæ•´è·¯å¾„: {module_full_name}")
            print(
                f"MoEç›¸å…³: Gate={is_moe_gate}, MoE={is_deepseek_moe}, æ½œåœ¨={is_potential_moe}"
            )

        # ä¸“é—¨å¤„ç†MoEGateçš„è¾“å‡ºæ ¼å¼
        if is_moe_gate and isinstance(output, tuple) and len(output) >= 2:
            expert_indices = None
            expert_weights = None

            # æ ¹æ®æ—¥å¿—åˆ†æï¼ŒMoEGateè¾“å‡ºæ ¼å¼ä¸ºï¼š(indices, weights, aux_loss)
            if len(output) >= 2:
                item0, item1 = output[0], output[1]

                # æ£€æŸ¥ç¬¬ä¸€ä¸ªè¾“å‡ºæ˜¯å¦æ˜¯ä¸“å®¶ç´¢å¼• (int64ç±»å‹)
                if hasattr(item0, "shape") and hasattr(item0, "dtype"):
                    if item0.dtype == torch.int64 and len(item0.shape) == 2:
                        expert_indices = item0
                        if should_log_detail:
                            print(
                                f"âœ… å‘ç°ä¸“å®¶ç´¢å¼•: shape={item0.shape}, dtype={item0.dtype}"
                            )

                # æ£€æŸ¥ç¬¬äºŒä¸ªè¾“å‡ºæ˜¯å¦æ˜¯ä¸“å®¶æƒé‡ (floatç±»å‹)
                if hasattr(item1, "shape") and hasattr(item1, "dtype"):
                    if (
                        item1.dtype in [torch.float32, torch.float16, torch.bfloat16]
                        and len(item1.shape) == 2
                    ):
                        expert_weights = item1
                        if should_log_detail:
                            print(
                                f"âœ… å‘ç°ä¸“å®¶æƒé‡: shape={item1.shape}, dtype={item1.dtype}"
                            )

            # å¦‚æœåŒæ—¶æ‰¾åˆ°ç´¢å¼•å’Œæƒé‡ï¼Œè®°å½•ä¸“å®¶æ¿€æ´»
            if expert_indices is not None and expert_weights is not None:
                try:
                    # è½¬æ¢ä¸ºPythonåˆ—è¡¨ä»¥ä¾¿åç»­å¤„ç†
                    indices_list = expert_indices.cpu().tolist()
                    weights_list = expert_weights.cpu().tolist()

                    expert_activations.append(
                        {
                            "module": module_name,
                            "full_name": module_full_name,
                            "hook_call": hook_call_count,
                            "expert_indices": indices_list,
                            "expert_weights": weights_list,
                            "indices_shape": list(expert_indices.shape),
                            "weights_shape": list(expert_weights.shape),
                            "type": "moe_gate_output",
                            "num_tokens": expert_indices.shape[0],
                            "experts_per_token": expert_indices.shape[1],
                        }
                    )

                    if should_log_detail:
                        print(f"ğŸ‰ æˆåŠŸè®°å½•MoEä¸“å®¶æ¿€æ´»!")
                        print(f"   Tokenæ•°é‡: {expert_indices.shape[0]}")
                        print(f"   æ¯ä¸ªtokençš„ä¸“å®¶æ•°: {expert_indices.shape[1]}")
                        # æ˜¾ç¤ºç¬¬ä¸€ä¸ªtokené€‰æ‹©çš„ä¸“å®¶
                        if len(indices_list) > 0 and len(indices_list[0]) > 0:
                            first_token_experts = indices_list[0]
                            first_token_weights = weights_list[0]
                            print(f"   ç¬¬ä¸€ä¸ªtokené€‰æ‹©çš„ä¸“å®¶: {first_token_experts}")
                            print(
                                f"   å¯¹åº”æƒé‡: {[f'{w:.4f}' for w in first_token_weights]}"
                            )

                except Exception as e:
                    if DEBUG_MODE:
                        print(f"âŒ å¤„ç†MoE Gateè¾“å‡ºæ—¶å‡ºé”™: {e}")

        # ç»§ç»­æ£€æŸ¥å…¶ä»–å¯èƒ½çš„MoEè¾“å‡ºæ ¼å¼ï¼ˆä¿æŒå…¼å®¹æ€§ï¼‰
        elif isinstance(output, tuple):
            for i, item in enumerate(output):
                if hasattr(item, "shape") and len(item.shape) >= 2:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯router logitsï¼ˆéœ€è¦softmaxçš„æ¦‚ç‡åˆ†å¸ƒï¼‰
                    if item.dtype in [torch.float32, torch.float16, torch.bfloat16]:
                        last_dim = item.shape[-1]
                        if 8 <= last_dim <= 256:  # åˆç†çš„ä¸“å®¶æ•°é‡èŒƒå›´
                            try:
                                # å°è¯•ä½œä¸ºrouter logitså¤„ç†
                                probs = F.softmax(item, dim=-1)
                                k = min(8, last_dim)
                                top_k_result = torch.topk(probs, k=k, dim=-1)
                                top_experts = top_k_result.indices.cpu().tolist()
                                top_probs = top_k_result.values.cpu().tolist()

                                expert_activations.append(
                                    {
                                        "module": module_name,
                                        "full_name": module_full_name,
                                        "hook_call": hook_call_count,
                                        "expert_indices": top_experts,
                                        "expert_probabilities": top_probs,
                                        "logits_shape": list(item.shape),
                                        "type": "router_logits",
                                    }
                                )

                                if should_log_detail:
                                    print(
                                        f"ğŸ¯ å‘ç°Router Logits: é¡¹{i}, shape={item.shape}"
                                    )
                                    print(f"   Topä¸“å®¶: {top_experts}")

                            except Exception as e:
                                if should_log_detail and DEBUG_MODE:
                                    print(f"âŒ å¤„ç†Router Logitsæ—¶å‡ºé”™: {e}")

        if should_log_detail:
            print("=" * 50)

    except Exception as e:
        if DEBUG_MODE:
            print(f"Hook #{hook_call_count} å¤„ç†é”™è¯¯: {e}")

# ========== Hookè®¾ç½®å‡½æ•° - ä¿ç•™åŸæœ‰é€»è¾‘ ==========
def setup_expert_hooks(model):
    """ä¸ºæ¨¡å‹è®¾ç½®ä¸“å®¶è¿½è¸ªhooks"""
    global hooks

    # æ¸…ç†ä¹‹å‰çš„hooks
    for hook in hooks:
        hook.remove()
    hooks = []

    hook_targets = []

    if DEBUG_MODE:
        print("ğŸ” åˆ†ææ¨¡å‹ç»“æ„...")

    # é¦–å…ˆæ‰“å°æ¨¡å‹çš„åŸºæœ¬ç»“æ„
    layer_types = {}
    for name, module in model.named_modules():
        module_type = type(module).__name__
        if module_type not in layer_types:
            layer_types[module_type] = []
        layer_types[module_type].append(name)

    if DEBUG_MODE:
        print("ğŸ“Š æ¨¡å‹ä¸­çš„å±‚ç±»å‹:")
        for layer_type, names in layer_types.items():
            print(f"  {layer_type}: {len(names)}ä¸ª")

    # ç­–ç•¥1: å¯»æ‰¾DeepSeek MoEç›¸å…³çš„å±‚
    moe_keywords = ["moe", "expert", "gate", "router", "ffn", "feed_forward"]

    for name, module in model.named_modules():
        module_name_lower = name.lower()
        # ä¸ºæ¯ä¸ªæ¨¡å—è®¾ç½®ä¸€ä¸ªæ ‡è¯†ç¬¦
        module._module_name = name

        # æ£€æŸ¥æ˜¯å¦æ˜¯MoEç›¸å…³å±‚
        if any(keyword in module_name_lower for keyword in moe_keywords):
            hook = module.register_forward_hook(detailed_track_experts)
            hooks.append(hook)
            hook_targets.append(name)

    # ç­–ç•¥2: å¦‚æœç­–ç•¥1æ²¡æ‰¾åˆ°è¶³å¤Ÿçš„å±‚ï¼Œæ‰©å¤§æœç´¢èŒƒå›´
    if len(hook_targets) < 5:
        if DEBUG_MODE:
            print("ğŸ”§ ç­–ç•¥1ç»“æœä¸è¶³ï¼Œæ‰©å¤§æœç´¢èŒƒå›´...")

        # å¯»æ‰¾å¯èƒ½åŒ…å«FFNæˆ–MLPçš„å±‚
        additional_keywords = ["mlp", "linear", "dense", "layer"]

        for name, module in model.named_modules():
            if name not in [target for target in hook_targets]:  # é¿å…é‡å¤
                module_name_lower = name.lower()
                module._module_name = name

                # æ£€æŸ¥æ˜¯å¦åŒ…å«FFNç›¸å…³çš„å±‚
                if any(keyword in module_name_lower for keyword in additional_keywords):
                    # é™åˆ¶hookæ•°é‡é¿å…è¿‡å¤šè¾“å‡º
                    if len(hook_targets) < 20:
                        hook = module.register_forward_hook(detailed_track_experts)
                        hooks.append(hook)
                        hook_targets.append(name)

    print(f"âœ… æ€»å…±æ³¨å†Œäº† {len(hook_targets)} ä¸ªHook")
    return len(hook_targets) > 0

# ========== ä¸“å®¶ç»Ÿè®¡å‡½æ•° ==========
def get_expert_info(max_records: int = 5):
    """è·å–ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯"""
    info = {
        "total_hooks": hook_call_count,
        "activation_records": len(expert_activations),
        "details": [],
        "usage": {},
        "summary": ""
    }

    # æ·»åŠ è¯¦ç»†è®°å½•
    for i, activation in enumerate(expert_activations[:max_records]):
        detail = {
            "module": activation["module"],
            "hook_call": activation["hook_call"],
            "type": activation.get("type", "unknown")
        }
        
        if "indices_shape" in activation:
            detail["shape"] = activation["indices_shape"]
            detail["experts"] = activation["expert_indices"]
        elif "logits_shape" in activation:
            detail["shape"] = activation["logits_shape"]
            detail["experts"] = activation["expert_indices"]
            
        info["details"].append(detail)

    # ç»Ÿè®¡ä¸“å®¶ä½¿ç”¨
    expert_usage = defaultdict(int)
    for activation in expert_activations:
        experts = activation.get("expert_indices", [])
        if isinstance(experts, list):
            for batch in experts:
                if isinstance(batch, list):
                    for token_experts in batch:
                        if isinstance(token_experts, list):
                            for expert_id in token_experts:
                                expert_usage[expert_id] += 1
                        else:
                            expert_usage[token_experts] += 1
                else:
                    expert_usage[batch] += 1

    info["usage"] = dict(sorted(expert_usage.items(), key=lambda x: x[1], reverse=True))
    
    # ç”Ÿæˆæ‘˜è¦
    if expert_usage:
        top_experts = list(info["usage"].keys())[:5]
        info["summary"] = f"å…±æ¿€æ´»{len(expert_activations)}æ¬¡ï¼Œä¸»è¦ä½¿ç”¨ä¸“å®¶: {top_experts}"
    else:
        info["summary"] = "æœªæ£€æµ‹åˆ°ä¸“å®¶æ¿€æ´»ä¿¡æ¯"

    return info

def reset_expert_tracking():
    """é‡ç½®ä¸“å®¶è¿½è¸ªçŠ¶æ€"""
    global expert_activations, hook_call_count
    expert_activations = []
    hook_call_count = 0

# ========== æ¨¡å‹åŠ è½½å‡½æ•° ==========
def load_model():
    global model, tokenizer
    
    print("æ­£åœ¨åŠ è½½æ¨¡å‹...")
    # åŠ è½½åˆ†è¯å™¨
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH, trust_remote_code=True
    )

    # åŠ è½½è¯­è¨€æ¨¡å‹
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        trust_remote_code=True,
        torch_dtype=torch.bfloat16,
        device_map="auto",
    )

    # åŠ è½½å¹¶è®¾ç½®ç”Ÿæˆé…ç½®
    model.generation_config = GenerationConfig.from_pretrained(MODEL_PATH)
    model.generation_config.pad_token_id = model.generation_config.eos_token_id
    model.eval()

    print("æ¨¡å‹åŠ è½½å®Œæˆ!")

    # è®¾ç½®ä¸“å®¶è¿½è¸ªhooks
    hooks_setup = setup_expert_hooks(model)
    if hooks_setup:
        print("âœ… ä¸“å®¶è¿½è¸ªåŠŸèƒ½å·²å¯ç”¨")
    else:
        print("âš ï¸ æœªæ‰¾åˆ°MoEç›¸å…³å±‚ï¼Œä¸“å®¶è¿½è¸ªå¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œ")

# ========== æ¨ç†å‡½æ•° ==========
def chat_generate(prompt_text, temperature=0.7, max_tokens=100, stream=False) -> Union[str, Generator]:
    """ç”ŸæˆèŠå¤©å›å¤"""
    if stream:
        def token_stream():
            # æ„å»ºè¾“å…¥
            messages = [{"role": "user", "content": prompt_text}]
            input_tensor = tokenizer.apply_chat_template(
                messages, add_generation_prompt=True, return_tensors="pt"
            )
            
            # ç”Ÿæˆå“åº”ID
            response_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
            created_time = int(time.time())
            
            # æµå¼ç”Ÿæˆ
            with torch.no_grad():
                # è¿™é‡Œéœ€è¦å®ç°çœŸæ­£çš„æµå¼ç”Ÿæˆ
                # ç”±äºtransformersçš„generateæ–¹æ³•ä¸ç›´æ¥æ”¯æŒæµå¼ï¼Œæˆ‘ä»¬å…ˆç”Ÿæˆå®Œæ•´ç»“æœç„¶åæ¨¡æ‹Ÿæµå¼
                generated = model.generate(
                    input_tensor.to(model.device),
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    do_sample=True,
                    pad_token_id=tokenizer.pad_token_id,
                    eos_token_id=tokenizer.eos_token_id,
                    use_cache=True
                )
            
            new_tokens = generated[:, input_tensor.shape[1]:]
            result = tokenizer.decode(new_tokens[0], skip_special_tokens=True)
            
            # å°†ç»“æœåˆ†æˆå•è¯è¿›è¡Œæµå¼è¾“å‡º
            words = result.split()
            
            for i, word in enumerate(words):
                chunk_data = {
                    "id": response_id,
                    "object": "chat.completion.chunk",
                    "created": created_time,
                    "model": "deepseek-moe-16b-chat",
                    "choices": [
                        {
                            "index": 0,
                            "delta": {
                                "content": word + " "
                            },
                            "finish_reason": None
                        }
                    ]
                }
                yield f"data: {json.dumps(chunk_data)}\n\n"
                time.sleep(0.05)  # æ¨¡æ‹Ÿæµå¼å»¶è¿Ÿ
            
            # å‘é€ç»“æŸæ ‡è®°
            final_chunk = {
                "id": response_id,
                "object": "chat.completion.chunk",
                "created": created_time,
                "model": "deepseek-moe-16b-chat",
                "choices": [
                    {
                        "index": 0,
                        "delta": {},
                        "finish_reason": "stop"
                    }
                ]
            }
            yield f"data: {json.dumps(final_chunk)}\n\n"
            yield "data: [DONE]\n\n"
        
        return token_stream()
        
    # éæµå¼ç”Ÿæˆ
    messages = [{"role": "user", "content": prompt_text}]
    input_tensor = tokenizer.apply_chat_template(
        messages, add_generation_prompt=True, return_tensors="pt"
    )
    
    with torch.no_grad():
        outputs = model.generate(
            input_tensor.to(model.device),
            max_new_tokens=max_tokens,
            temperature=temperature,
            do_sample=True,
            pad_token_id=tokenizer.pad_token_id,
            eos_token_id=tokenizer.eos_token_id,
            use_cache=True
        )
    
    result = tokenizer.decode(
        outputs[0][input_tensor.shape[1]:], skip_special_tokens=True
    )
    return result

# ========== å¯åŠ¨äº‹ä»¶ ==========
@app.on_event("startup")
def startup_event():
    load_model()

# ========== APIç«¯ç‚¹ ==========

# 1. OpenAIæ ‡å‡†API
@app.post("/v1/chat/completions")
def chat_completions(req: ChatRequest):
    reset_expert_tracking()
    
    # æ„å»ºå¯¹è¯æ–‡æœ¬
    messages = [m.dict() for m in req.messages]
    if len(messages) > 0:
        prompt = messages[-1]["content"]  # ç®€åŒ–å¤„ç†ï¼Œå–æœ€åä¸€æ¡æ¶ˆæ¯
    else:
        prompt = ""
    
    if req.stream:
        stream = chat_generate(prompt, req.temperature, req.max_tokens, stream=True)
        return StreamingResponse(stream, media_type="text/event-stream")
    
    result = chat_generate(prompt, req.temperature, req.max_tokens)
    response_id = f"chatcmpl-{uuid.uuid4().hex[:8]}"
    print(f'å“åº”ï¼š{result}')
    print(get_expert_info())

    return {
        "id": response_id,
        "object": "chat.completion",
        "created": int(time.time()),
        "model": req.model,
        "choices": [
            {
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": result
                },
                "finish_reason": "stop"
            }
        ],
        "usage": {
            "prompt_tokens": len(tokenizer.encode(prompt)),
            "completion_tokens": len(tokenizer.encode(result)),
            "total_tokens": len(tokenizer.encode(prompt)) + len(tokenizer.encode(result))
        },
        "expert_info": get_expert_info()
    }

# 2. å…¼å®¹åŸæœ‰APIæ ¼å¼
@app.post("/")
async def legacy_endpoint(request: Request):
    """ä¿æŒä¸åŸapi6(3).pyå…¼å®¹çš„ç«¯ç‚¹"""
    try:
        json_post_raw = await request.json()
        json_post = json.dumps(json_post_raw)
        json_post_list = json.loads(json_post)
        prompt = json_post_list.get("prompt")
        max_length = json_post_list.get("max_length", 512)

        # é‡ç½®ä¸“å®¶è¿½è¸ªçŠ¶æ€
        reset_expert_tracking()

        # ç”Ÿæˆå›å¤
        result = chat_generate(prompt, max_tokens=max_length)

        now = datetime.datetime.now()
        time_str = now.strftime("%Y-%m-%d %H:%M:%S")

        # æ„å»ºå“åº”JSON
        answer = {
            "response": result,
            "status": 200,
            "time": time_str,
            "expert_info": get_expert_info()  # æ·»åŠ ä¸“å®¶ä¿¡æ¯
        }

        # æ„å»ºæ—¥å¿—ä¿¡æ¯
        log = (
            "["
            + time_str
            + "] "
            + 'prompt:"'
            + prompt
            + '", response:"'
            + repr(result)
            + '"'
        )
        print(log)
        torch_gc()

        return answer

    except Exception as e:
        print(f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        now = datetime.datetime.now()
        time_str = now.strftime("%Y-%m-%d %H:%M:%S")
        return {
            "response": f"å¤„ç†è¯·æ±‚æ—¶å‘ç”Ÿé”™è¯¯: {str(e)}",
            "status": 500,
            "time": time_str,
        }

# 3. å¥åº·æ£€æŸ¥
@app.get("/health")
def health():
    return {
        "status": "ok",
        "device": CUDA_DEVICE,
        "torch_version": torch.__version__,
        "model_loaded": model is not None,
        "hooks_registered": len(hooks)
    }

# 4. ä¸“å®¶ä¿¡æ¯æŸ¥è¯¢
@app.get("/expert/info")
def get_expert_statistics():
    return get_expert_info(max_records=10)

# 5. æ¨¡å‹ç»“æ„è¯Šæ–­
@app.get("/debug/model_structure")
def get_model_structure():
    if model is None:
        return {"error": "æ¨¡å‹æœªåŠ è½½"}
    
    total_modules = 0
    moe_related = []
    
    for name, module in model.named_modules():
        total_modules += 1
        if any(k in name.lower() for k in ['moe', 'expert', 'gate', 'router']):
            moe_related.append({
                "name": name,
                "type": type(module).__name__
            })
    
    return {
        "total_modules": total_modules,
        "moe_related_count": len(moe_related),
        "moe_modules": moe_related[:20],  # åªè¿”å›å‰20ä¸ª
        "hooks_registered": len(hooks)
    }

# 6. è·å–æ¨¡å‹ä¿¡æ¯
@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨æ¨¡å‹ï¼ˆå…¼å®¹OpenAI APIæ ¼å¼ï¼‰"""
    return {
        "object": "list",
        "data": [
            {
                "id": "deepseek-moe-16b",
                "object": "model",
                "created": 1751604676,
                "owned_by": "vllm",
                "root": "models/deepseek-moe-16b",
                "parent": None,
                "max_model_len": 4096,
                "permission": [
                    {
                        "id": "modelperm-132753c82dfa4e1a966dea5a0100ace9",
                        "object": "model_permission",
                        "created": 1751604676,
                        "allow_create_engine": False,
                        "allow_sampling": True,
                        "allow_logprobs": True,
                        "allow_search_indices": False,
                        "allow_view": True,
                        "allow_fine_tuning": False,
                        "organization": "*",
                        "group": None,
                        "is_blocking": False
                    }
                ]
            }
        ]
    }

# ========== å‘½ä»¤è¡Œå¯åŠ¨å…¥å£ ==========
def main():
    global DEBUG_MODE
    import uvicorn

    # å‘½ä»¤è¡Œå‚æ•°è§£æ
    parser = argparse.ArgumentParser(description="å¯åŠ¨ DeepSeek MoE API Server")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨è°ƒè¯•æ¨¡å¼")
    parser.add_argument("--port", type=int, default=6006, help="è®¾ç½®è¿è¡Œç«¯å£ï¼Œé»˜è®¤ä¸º6006")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="è®¾ç½®ç»‘å®šåœ°å€ï¼Œé»˜è®¤ä¸º0.0.0.0")
    args = parser.parse_args()

    DEBUG_MODE = args.debug
    port = args.port
    host = args.host

    print("ğŸš€ å¯åŠ¨ DeepSeek MoE API Server")
    print(f"ğŸ“ åœ°å€: http://{host}:{port}")
    print(f"ğŸ”§ è°ƒè¯•æ¨¡å¼: {'å¼€å¯' if DEBUG_MODE else 'å…³é—­'}")
    print(f"ğŸ“Š æ”¯æŒç«¯ç‚¹:")
    print(f"   - POST /v1/chat/completions (OpenAIæ ‡å‡†)")
    print(f"   - POST / (å…¼å®¹åŸAPI)")
    print(f"   - GET /health (å¥åº·æ£€æŸ¥)")
    print(f"   - GET /expert/info (ä¸“å®¶ä¿¡æ¯)")
    print(f"   - GET /debug/model_structure (æ¨¡å‹ç»“æ„)")

    uvicorn.run("moe_server:app", host=host, port=port, workers=1, reload=False)

if __name__ == "__main__":
    main()